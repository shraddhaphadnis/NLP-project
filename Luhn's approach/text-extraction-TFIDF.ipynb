{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_summary.csv')\n",
    "i = 3120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = df['read_more'][i]\n",
    "gold_summary = df['text'][i]\n",
    "if \"indiatoday\" in url or \"intoday\" in url:\n",
    "    baseurl = \"i\"\n",
    "elif \"hindustantimes\" in url:\n",
    "    baseurl = \"h\"\n",
    "elif \"theguardian\" in url:\n",
    "    baseurl = \"t\"\n",
    "file = urllib.urlopen(url)\n",
    "html = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Parse the data to extract article text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "if baseurl == \"i\":\n",
    "    entries = soup.find_all('div',{'class':'description'})\n",
    "elif baseurl == \"t\":\n",
    "    entries = soup.find_all('div',{'itemprop':'articleBody'})\n",
    "elif baseurl == \"h\":\n",
    "    entries = soup.find_all('div',{'itemprop':'articlebody'})\n",
    "\n",
    "for each in entries:\n",
    "        if each.figure:\n",
    "            each.figure.decompose()\n",
    "content = []\n",
    "for e in entries:\n",
    "     content.extend(e.find_all(\"p\"))\n",
    "\n",
    "text = \"\"\n",
    "for each in content:\n",
    "    text = text + each.get_text() +\" \"\n",
    "    \n",
    "text = text.encode('utf-8').decode(\"unicode_escape\").encode('ascii','ignore')\n",
    "print text\n",
    "text = nltk.sent_tokenize(text)    \n",
    "    \n",
    "# for each in text:\n",
    "#     print each , \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize the article\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_list=[]\n",
    "for t in text:\n",
    "    word_list.extend(tokenizer.tokenize(t))\n",
    "\n",
    "\n",
    "# remove stop words from the article\n",
    "filtered_words = [word for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "total_words = len(filtered_words)\n",
    "words = filtered_words\n",
    "\n",
    "# find n-gram probability for filtered words\n",
    "filtered_words  = []\n",
    "for each in Counter(words).items():\n",
    "    filtered_words.append([each[0] , float(each[1])/float(total_words)])\n",
    "\n",
    "# identify important words\n",
    "important_words = []\n",
    "for item in filtered_words:\n",
    "    if item[1]>0.003:\n",
    "        important_words.append(item[0])\n",
    "\n",
    "while '. ' in text:\n",
    "    text = text.replace('. ','\\n') \n",
    "# calculate sentence score\n",
    "while \"\\n\" in text:\n",
    "    text = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(text)\n",
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = zip(vect.get_feature_names(),\n",
    "                 np.asarray(dtm.sum(axis=0)).ravel())\n",
    "sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "# for item in sorted_scores:\n",
    "#     print \"{0:50} Score: {1}\".format(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# #review_id = np.random.randint(0, len(text))\n",
    "# review_text = words\n",
    "# review_length = len(words)\n",
    "\n",
    "# # create a dictionary of words and their TF-IDF scores\n",
    "# word_scores = {}\n",
    "# for word in words:\n",
    "# #     word = word.lower()\n",
    "#     if word in features:\n",
    "# #         print word\n",
    "# #         print dtm[review_id, features.index(word)]\n",
    "#          #word_scores[word] = dtm[review_id, features.index(word)]\n",
    "#          word_scores[word] = dtm[10, features.index(word)]\n",
    "\n",
    "# print word_scores\n",
    "# # print words with the top 5 TF-IDF scores\n",
    "# print 'TOP SCORING WORDS:'\n",
    "# top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "# for word, score in top_scores:\n",
    "#     print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_score = []\n",
    "for each in text:\n",
    "    score = 0.0\n",
    "#     each = each.\n",
    "    for word in each.split(\" \"):\n",
    "        #print word\n",
    "        word = word.lower()\n",
    "        if word in sorted_scores:\n",
    "#             print \"Hello\"\n",
    "            score += sorted_scores[word]\n",
    "#             print word_scores[word]\n",
    "    sentence_score.append([each,score])    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "machine_summary = \"\"\n",
    "sentences = []\n",
    "for each in sentence_score:\n",
    "    if each[1] != 0.0:\n",
    "        machine_summary = machine_summary + each[0]\n",
    "        sentences.append([each[0],each[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rougeN(gold_summary,machine_summary,n):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    gold_summary_slammed = \"\"\n",
    "    for word in gold_summary.split():\n",
    "        gold_summary_slammed = gold_summary_slammed+stemmer.stem(word)+\" \"\n",
    "\n",
    "\n",
    "    machine_summary_slammed = \"\"\n",
    "    for word in machine_summary.split():\n",
    "        machine_summary_slammed = machine_summary_slammed+stemmer.stem(word)+\" \"\n",
    "\n",
    "    n_gold = ngrams(gold_summary_slammed.split(\" \"),n)\n",
    "    n_machine = ngrams(machine_summary_slammed.split(\" \"),n)\n",
    "\n",
    "    gold_list = []\n",
    "\n",
    "    for gram in n_gold:\n",
    "        gold_list.append(gram)\n",
    "\n",
    "    machine_list = []\n",
    "    for gram in n_machine:\n",
    "        machine_list.append(gram)\n",
    "\n",
    "    return float(len(list(set(gold_list).intersection(machine_list))))/float(len(list(set(gold_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for SUMMARY_COMP_FACT in range(1,len(sentences)):\n",
    "    top_sentences = sorted(dict(sentences).items(), key=lambda x: x[1], reverse=True)[:(len(sentences)/SUMMARY_COMP_FACT)]\n",
    "    machine_summary = \"\"\n",
    "    for each in top_sentences:\n",
    "#         print each\n",
    "        machine_summary = machine_summary + each[0]\n",
    "    scores.append([SUMMARY_COMP_FACT, len(top_sentences), rougeN(gold_summary,machine_summary,2), rougeN(gold_summary,machine_summary,1)])\n",
    "        #print top_sentences[each],each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scores1 = np.array(scores)\n",
    "# print scores1\n",
    "# # val = np.average(scores1[:,2])\n",
    "# for i in range(0,len(scores1[:])-1):\n",
    "#     diff = scores1[i,2]-scores1[i+1,2]\n",
    "#     print diff, scores1[i,1]\n",
    "#     if (diff > 0.15):\n",
    "#         num_sen =  scores1[i,1]\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary = sorted(dict(sentences).items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "# for s in summary:\n",
    "#     print s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salman Khan will never move out of his flat into a bigger bungalow because he has many memories attached to his home. \n",
      "\n",
      "At 51, Salman is now tackling yet another 'panga': Tiger Zinda Hai and Remo D'Souza's dance film. \n",
      "\n",
      "Salman said, \"After Sultan, there was lot of pain in my body... Now I have signed Tiger Zinda Hai like a fool, I am jumping off the building, running and shooting, doing all action; I am going mad in the film. \n",
      "\n",
      "Contrary to reports, Salman Khan has not signed ABCD 3 opposite Jacqueline Fernandez. \n",
      "\n",
      "Even though Salman Khan and his Tubelight director Kabir Khan share a great rapport, Salman says he will not accept any and every film that Kabir gives him. \n",
      "\n",
      "5 intriguing points from this Salman film ALSO WATCH: A look at Salman Khan's controversial life \n",
      "\n",
      "0.314814814815 0.571428571429\n"
     ]
    }
   ],
   "source": [
    "# arr = np.array(sentences)\n",
    "# arr\n",
    "paired_sens = {}\n",
    "num_sen = len(sentences)/8\n",
    "\n",
    "for pair in enumerate(sentences):\n",
    "     paired_sens[pair[0]]=pair[1][1]\n",
    "        \n",
    "temp = sorted((paired_sens).items(), key=lambda x: x[1], reverse=True)[:int(num_sen)]\n",
    "temp = sorted(dict(temp).items(), key=lambda x: x[0], reverse=False)[:int(num_sen)]\n",
    "machine_summary = \"\"\n",
    "for i in temp:\n",
    "    print sentences[i[0]][0],\"\\n\"\n",
    "    machine_summary = machine_summary +  sentences[i[0]][0] + \" \"\n",
    "print rougeN(gold_summary,machine_summary,2),rougeN(gold_summary,machine_summary,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def find_nearest(array,value):\n",
    "#     idx = (np.abs(array-value)).argmin()\n",
    "#     return idx,array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUMMARY_COMP_FACT = find_nearest(scores1[:,2],val)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top_sentences = sorted(dict(sentences).items(), key=lambda x: x[1], reverse=True)[:(len(sentences)/SUMMARY_COMP_FACT)]\n",
    "# machine_summary = \"\"\n",
    "# for each in top_sentences:\n",
    "#     print each\n",
    "#     machine_summary = machine_summary + each[0]\n",
    "# print SUMMARY_COMP_FACT, len(top_sentences), rougeN(gold_summary,machine_summary,2), rougeN(gold_summary,machine_summary,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# machine_summary = \"An alleged suspect in a kidnapping case was found hanging inside the washroom of the Jahangirpuri police station in north Delhi on Wednesday, hours after he was called by the cops for interrogation. Their relationship ended a few months ago and Kumar quit the job and returned to his village, said a police officer. The woman went missing a few months ago, prompting her husband to file a police complaint and a habeas corpus petition at the Delhi High Court. A team was sent to Kumars village but when he was found unavailable, he was instructed to report to the Jahangirpuri police station. He willingly arrived in Delhi on Tuesday and visited the police station, said Kumars friend, Dharmendra, who dropped him at the police station around 3 pm.\"\n",
    "# print rougeN(gold_summary,machine_summary,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
