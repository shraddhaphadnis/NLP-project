{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read article from file\n",
    "f = open(\"article\",'r')\n",
    "text =  f.read()\n",
    "f.close()\n",
    "\n",
    "# tokenize the article\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_list = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "# remove stop words from the article\n",
    "filtered_words = [word for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "total_words = len(filtered_words)\n",
    "words = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find n-gram probability for filtered words\n",
    "filtered_words  = []\n",
    "for each in Counter(words).items():\n",
    "    filtered_words.append([each[0],float(each[1])/float(total_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count bigram of the given article\n",
    "def ngrams_computation(input,n):\n",
    "    return zip(*[input[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = ngrams_computation(words, 2)\n",
    "#print grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# identify important words\n",
    "important_words = []\n",
    "for item in filtered_words:\n",
    "    if item[1]>0.003:\n",
    "        important_words.append(item[0])\n",
    "\n",
    "# calculate sentence score\n",
    "while \"\\n\" in text:\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "sentenceScore  = []\n",
    "for sentence in text:\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for word in important_words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            count = count +1\n",
    "        if word in sentence and word not in stopwords.words('english'):\n",
    "            score = score + 1\n",
    "    sentenceScore.append([sentence,float(score)/float(count)**(1/2)])\n",
    "\n",
    "# extract top n/5 sentences (n is total number of sentences in the text)\n",
    "sen = sentenceScore\n",
    "sentenceScore.sort(key=lambda x: x[1], reverse=True)\n",
    "print len(sentenceScore)\n",
    "cutScore = sentenceScore[len(sentenceScore)/5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "summarized_output = \"\"\n",
    "for sentence in sen:\n",
    "    if sentence[1]>cutScore:\n",
    "        summarized_output += sentence[0]\n",
    "        #print sentence[0] + \"\\n\"\n",
    "        count = count + 1\n",
    "#print count\n",
    "#print summarized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'at', 'least', '2', '000', 'people', 'who', 'lost', 'their', 'limbs', 'in', 'the', 'Haiti', 'earthquake', 'a', 'figure', 'far', 'higher', 'than', 'a', 'similar', 'disaster', 'in', 'Pakistan', 'five', 'years', 'ago', 'explains', 'Celia', 'Du', 'Pre', 'from', 'Handicap', 'International', 'For', 'the', 'moment', 'the', 'scale', 'of', 'the', 'disaster', 'has', 'made', 'amputations', 'big', 'news', 'but', 'Dr', 'Chamouilles', 'is', 'among', 'the', 'many', 'who', 'fear', 'that', 'without', 'long', 'term', 'international', 'support', 'it', 'will', 'slip', 'off', 'the', 'agenda', 'and', 'Haiti', 's', 'handicapped', 'survivors', 'will', 'be', 'forgotten', 'Emmanuel', 'Etienne', 'was', 'terrified', 'he', 'would', 'lose', 'his', 'leg', 'Emmanuel', 'Etienne', 'may', 'be', 'badly', 'injured', 'in', 'a', 'field', 'hospital', 'set', 'up', 'on', 'an', 'old', 'tennis', 'court', 'but', 'he', 'feels', 'like', 'a', 'lucky', 'man', 'Some', '2', '000', 'Haitian', 'earthquake', 'survivors', 'have', 'had', 'limbs', 'amputated', 'Charity', 'workers', 'are', 'handing', 'out', 'crutches', 'to', 'patients', 'as', 'they', 'leave', 'But', 'many', 'veterans', 'of', 'disasters', 'fear', 'the', 'scale', 'of', 'this', 'earthquake', 'will', 'overwhelm', 'such', 'well', 'meaning', 'organizations', 'because', 'the', 'numbers', 'needing', 'help', 'are', 'simply', 'so', 'great', 'It', 's', 'hard', 'enough', 'in', 'Haiti', 'having', 'both', 'of', 'them', 'let', 'alone', 'just', 'one', 'I', 'do', 'n', 't', 'think', 'I', 'would', 'have', 'lived', 'very', 'long']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the summary\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "summary_word_list = tokenizer.tokenize(summarized_output)\n",
    "#print summary_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find bigrams in the summarized output\n",
    "summarized_output_bigram = ngrams_computation(summary_word_list, 2)\n",
    "#print summarized_output_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.156479217604\n"
     ]
    }
   ],
   "source": [
    "for gram in grams:\n",
    "    if gram in bigrams:\n",
    "        bigrams[gram] += 1\n",
    "    else:\n",
    "        bigrams[gram] = 1\n",
    "    \n",
    "#print bigrams    \n",
    "numerator = 0\n",
    "for each in summarized_output_bigram:\n",
    "    if each in bigrams:\n",
    "        numerator += bigrams[each]\n",
    "denominator = sum(bigrams.values())\n",
    "\n",
    "Rouge_2 = float(numerator)/denominator\n",
    "print Rouge_2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
