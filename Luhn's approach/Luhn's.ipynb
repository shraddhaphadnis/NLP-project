{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashwat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# read article from file\n",
    "f = open(\"article\",'r')\n",
    "text =  f.read().decode('utf-8','ignore').encode(\"utf-8\")\n",
    "f.close()\n",
    "\n",
    "# tokenize the article\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_list = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "# remove stop words from the article\n",
    "filtered_words = [word for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "total_words = len(filtered_words)\n",
    "words = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find n-gram probability for filtered words\n",
    "filtered_words  = []\n",
    "for each in Counter(words).items():\n",
    "    filtered_words.append([each[0],float(each[1])/float(total_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count bigram of the given article\n",
    "def ngrams_computation(input,n):\n",
    "    return zip(*[input[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = ngrams_computation(words, 2)\n",
    "#print grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashwat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/home/shashwat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:20: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# identify important words\n",
    "important_words = []\n",
    "for item in filtered_words:\n",
    "    if item[1]>0.003:\n",
    "        important_words.append(item[0])\n",
    "while \". \" in text:\n",
    "    text = text.replace(\". \",\"\\n\")\n",
    "    \n",
    "# calculate sentence score\n",
    "while \"\\n\" in text:\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "sentenceScore  = []\n",
    "for sentence in text:\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for word in important_words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            count = count +1\n",
    "        if word in sentence and word not in stopwords.words('english'):\n",
    "            score = score + 1\n",
    "    sentenceScore.append([sentence,float(score)/float(count)**(1/2)])\n",
    "\n",
    "# extract top n/5 sentences (n is total number of sentences in the text)\n",
    "sen = sentenceScore\n",
    "sentenceScore.sort(key=lambda x: x[1], reverse=True)\n",
    "print len(sentenceScore)\n",
    "cutScore = sentenceScore[len(sentenceScore)/5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "summarized_output = \"\"\n",
    "for sentence in sen:\n",
    "    if sentence[1]>cutScore:\n",
    "        summarized_output += sentence[0]\n",
    "        #print sentence[0] + \"\\n\"\n",
    "        count = count + 1\n",
    "#print count\n",
    "#print summarized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the summary\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "summary_word_list = tokenizer.tokenize(summarized_output)\n",
    "#print summary_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In',), ('this',), ('connection',), ('all',), ('offices',), ('departments',), ('shall',), ('remain',), ('open',), ('and',), ('celebrate',), ('the',), ('festival',), ('collectively',), ('at',), ('a',), ('suitable',), ('time',), ('wherein',), ('all',), ('the',), ('lady',), ('staff',), ('shall',), ('tie',), ('rakhis',), ('to',), ('their',), ('colleagues',), ('\\xe2',), ('the',), ('order',), ('issued',), ('on',), ('August',), ('1',), ('by',), ('Gurpreet',), ('Singh',), ('deputy',), ('secretary',), ('personnel',), ('had',), ('said',), ('In',), ('2014',), ('the',), ('year',), ('BJP',), ('stormed',), ('to',), ('power',), ('at',), ('the',), ('Centre',), ('Rashtriya',), ('Swayamsevak',), ('Sangh',), ('RSS',), ('chief',), ('Mohan',), ('Bhagwat',), ('said',), ('the',), ('festival',), ('had',), ('\\xe2',), ('national',), ('significance\\xe2',), ('and',), ('should',), ('be',), ('celebrated',), ('widely',), ('\\xe2',), ('to',), ('protect',), ('Hindu',), ('culture',), ('and',), ('live',), ('by',), ('the',), ('values',), ('enshrined',), ('in',), ('it\\xe2',), ('The',), ('Daman',), ('and',), ('Diu',), ('administration',), ('on',), ('Wednesday',), ('withdrew',), ('a',), ('circular',), ('that',), ('asked',), ('women',), ('staff',), ('to',), ('tie',), ('rakhis',), ('on',), ('male',), ('colleagues',), ('after',), ('the',), ('order',), ('triggered',), ('a',), ('backlash',), ('from',), ('employees',), ('and',), ('was',), ('ripped',), ('apart',), ('on',), ('social',), ('media',), ('Rakshabandhan',), ('a',), ('celebration',), ('of',), ('the',), ('bond',), ('between',), ('brothers',), ('and',), ('sisters',), ('is',), ('one',), ('of',), ('several',), ('Hindu',), ('festivities',), ('and',), ('rituals',), ('that',), ('are',), ('no',), ('longer',), ('confined',), ('of',), ('private',), ('family',), ('affairs',), ('but',), ('have',), ('become',), ('tools',), ('to',), ('push',), ('politic',), ('al',), ('ideologies',), ('The',), ('two',), ('notifications',), ('\\xe2',), ('one',), ('mandating',), ('the',), ('celebration',), ('of',), ('Rakshabandhan',), ('left',), ('and',), ('the',), ('other',), ('withdrawing',), ('the',), ('mandate',), ('right',), ('\\xe2',), ('were',), ('issued',), ('by',), ('the',), ('Daman',), ('and',), ('Diu',), ('administration',), ('a',), ('day',), ('apart',), ('How',), ('can',), ('the',), ('government',), ('dictate',), ('who',), ('I',), ('should',), ('tie',), ('rakhi',), ('to',), ('We',), ('should',), ('maintain',), ('the',), ('professionalism',), ('of',), ('a',), ('workplace\\xe2',), ('an',), ('official',), ('told',), ('Hindustan',), ('Times',), ('earlier',), ('in',), ('the',), ('day',)]\n"
     ]
    }
   ],
   "source": [
    "# # find bigrams in the summarized output\n",
    "summarized_output_bigram = ngrams_computation(summary_word_list, 1)\n",
    "print summarized_output_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41666666667\n"
     ]
    }
   ],
   "source": [
    "bigrams = {}\n",
    "for gram in grams:\n",
    "    if gram in bigrams:\n",
    "        bigrams[gram] += 1\n",
    "    else:\n",
    "        bigrams[gram] = 1\n",
    "    \n",
    "#print bigrams    \n",
    "numerator = 0\n",
    "for each in summarized_output_bigram:\n",
    "    if each in bigrams:\n",
    "        numerator += bigrams[each]\n",
    "denominator = sum(bigrams.values())\n",
    "\n",
    "Rouge_2 = float(numerator)/denominator\n",
    "print Rouge_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
