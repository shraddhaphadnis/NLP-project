{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "df = pd.read_csv('news_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(html,baseurl):\n",
    "    soup = bs(html, 'html.parser')\n",
    "    if baseurl == 'i':\n",
    "        entries = soup.find_all('div',{'class':'description'})\n",
    "    elif baseurl == 't':\n",
    "        entries = soup.find_all('div',{'itemprop':'articleBody'})\n",
    "    elif baseurl == 'h':\n",
    "        entries = soup.find_all('div',{'itemprop':'articlebody'})\n",
    "    for each in entries:\n",
    "            if each.figure:\n",
    "                each.figure.decompose()\n",
    "    content = []\n",
    "    for e in entries:\n",
    "         content.extend(e.find_all(\"p\"))\n",
    "    \n",
    "    text = \"\"\n",
    "    for each in content:\n",
    "        text = text + each.get_text() +\" \"\n",
    "\n",
    "    text = text.encode('utf-8').decode(\"unicode_escape\").encode('ascii','ignore')\n",
    "#     text = nltk.sent_tokenize(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readUrl(url):\n",
    "    if \"indiatoday\" in url or \"intoday\" in url:\n",
    "        baseurl = \"i\"\n",
    "    elif \"hindustantimes\" in url:\n",
    "        baseurl = \"h\"\n",
    "    elif \"theguardian\" in url:\n",
    "        baseurl = \"t\"\n",
    "\n",
    "    file = urllib.urlopen(url)\n",
    "    html = file.read()\n",
    "\n",
    "    text = clean_data(html,baseurl)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "df1 = list(df['read_more'])\n",
    "df1 = df1[:1]\n",
    "len (df1)\n",
    "for doc in df1:\n",
    "#      print doc\n",
    "     data.append(readUrl(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc.split(),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument, LabeledSentence, Doc2Vec\n",
    "\n",
    "it = LabeledLineSentence(data, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec(vector_size=300, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025) # use fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "model.build_vocab(it)\n",
    "for epoch in range(10):\n",
    "    model.train(it,total_examples = model.corpus_count,epochs = model.epochs)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no deca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x7f3d486ffa50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
