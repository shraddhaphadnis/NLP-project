{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "df = pd.read_csv('news_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(html,baseurl):\n",
    "    soup = bs(html, 'html.parser')\n",
    "    if baseurl == 'i':\n",
    "        entries = soup.find_all('div',{'class':'description'})\n",
    "    elif baseurl == 't':\n",
    "        entries = soup.find_all('div',{'itemprop':'articleBody'})\n",
    "    elif baseurl == 'h':\n",
    "        entries = soup.find_all('div',{'itemprop':'articlebody'})\n",
    "    for each in entries:\n",
    "            if each.figure:\n",
    "                each.figure.decompose()\n",
    "    content = []\n",
    "    for e in entries:\n",
    "         content.extend(e.find_all(\"p\"))\n",
    "    \n",
    "    text = \"\"\n",
    "    for each in content:\n",
    "        text = text + each.get_text() +\" \"\n",
    "\n",
    "    text = text.encode('utf-8').decode(\"unicode_escape\").encode('ascii','ignore')\n",
    "#     text = nltk.sent_tokenize(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readUrl(url):\n",
    "    if \"indiatoday\" in url or \"intoday\" in url:\n",
    "        baseurl = \"i\"\n",
    "    elif \"hindustantimes\" in url:\n",
    "        baseurl = \"h\"\n",
    "    elif \"theguardian\" in url:\n",
    "        baseurl = \"t\"\n",
    "\n",
    "    file = urllib.urlopen(url)\n",
    "    html = file.read()\n",
    "\n",
    "    text = clean_data(html,baseurl)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "df1 = list(df['read_more'])\n",
    "df1 = df1[:20]\n",
    "len (df1)\n",
    "for doc in df1:\n",
    "#      print doc\n",
    "     data.append(readUrl(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc.split(),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument, LabeledSentence, Doc2Vec\n",
    "\n",
    "it = LabeledLineSentence(data, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec(vector_size=300, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025) # use fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "model.build_vocab(it)\n",
    "for epoch in range(10):\n",
    "    model.train(it,total_examples = model.corpus_count,epochs = model.epochs)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no deca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Doc2VecKeyedVectors at 0x7f3d47b28350>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from gensim.models import Word2Vec,Doc2Vec\n",
    "\n",
    "\n",
    "\n",
    "w2v_model_filename = \"../dataset/w2v_model\"\n",
    "\n",
    "class Sen2VecByWord2Vec(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w2v_model = Word2Vec.load(w2v_model_filename)\n",
    "\n",
    "    def _w2v(self,word):\n",
    "        try:\n",
    "            return self.w2v_model[word]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def sen2vec(self,sentence):\n",
    "        distributed_words = []\n",
    "        for word in sentence:\n",
    "            ret = self._w2v(word)\n",
    "            if ret is not None:\n",
    "                distributed_words.append(ret)\n",
    "\n",
    "        if (len(distributed_words) ):\n",
    "            return numpy.array(distributed_words).astype(\"float32\").mean(axis=0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def sens2vec(self,sentences):\n",
    "        sens = []\n",
    "        for sentence in sentences:\n",
    "            sens.append(self.sen2vec(sentence))\n",
    "\n",
    "        return sens\n",
    "\n",
    "\n",
    "d2v_model_filename = \"../dataset/d2v_model\"\n",
    "\n",
    "class Sen2VecByDoc2Vec(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.d2v_model = Doc2Vec.load(d2v_model_filename)\n",
    "\n",
    "\n",
    "    def sen2vec(self,sentence):\n",
    "        return self.d2v_model.infer_vector(sentence,steps=100)\n",
    "\n",
    "    def sens2vec(self,sentences):\n",
    "        sens = []\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            print str(i) + \"\\r\",\n",
    "            sens.append(self.sen2vec(sentence))\n",
    "        print \"\"\n",
    "\n",
    "return sens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
