{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>News article Summarization</h1>\n",
    "<p>This file contains a main function to perform tet summarization. The main function performs summarization with three methods Luhn's approach with TF-IDF and Stemming, TextRank and TextRank with Word2Vec. The rouge-1 and rouge2 score for each file of the dataset 'news_summaray.csv' is calculated and stored in a file.</p>\n",
    "\n",
    "<h2>Import Dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Parse and clean data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(html,baseurl):\n",
    "    soup = bs(html, 'html.parser')\n",
    "    if baseurl == 'i':\n",
    "        entries = soup.find_all('div',{'class':'description'})\n",
    "    elif baseurl == 't':\n",
    "        entries = soup.find_all('div',{'itemprop':'articleBody'})\n",
    "    elif baseurl == 'h':\n",
    "        entries = soup.find_all('div',{'itemprop':'articlebody'})\n",
    "    for each in entries:\n",
    "            if each.figure:\n",
    "                each.figure.decompose()\n",
    "    content = []\n",
    "    for e in entries:\n",
    "         content.extend(e.find_all(\"p\"))\n",
    "    \n",
    "    text = \"\"\n",
    "    for each in content:\n",
    "        text = text + each.get_text() +\" \"\n",
    "\n",
    "    text = text.encode('utf-8').decode(\"unicode_escape\").encode('ascii','ignore')\n",
    "    text = nltk.sent_tokenize(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Luhn's Approach </h2>\n",
    "<h3> Extraction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_scores(text):\n",
    "    # tokenize the article\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_list=[]\n",
    "    for t in text:\n",
    "        word_list.extend(tokenizer.tokenize(t))\n",
    "\n",
    "\n",
    "    # remove stop words from the article\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "    total_words = len(filtered_words)\n",
    "    words = filtered_words\n",
    "\n",
    "    # find n-gram probability for filtered words\n",
    "    filtered_words  = []\n",
    "    for each in Counter(words).items():\n",
    "        filtered_words.append([each[0] , float(each[1])/float(total_words)])\n",
    "\n",
    "    # identify important words\n",
    "    important_words = []\n",
    "    for item in filtered_words:\n",
    "        if item[1]>0.003:\n",
    "            important_words.append(item[0])\n",
    "\n",
    "    while '. ' in text:\n",
    "        text = text.replace('. ','\\n') \n",
    "\n",
    "    while \"\\n\" in text:\n",
    "        text = text.split(\"\\n\")\n",
    "        \n",
    "    vect = TfidfVectorizer(stop_words='english')\n",
    "    dtm = vect.fit_transform(text)\n",
    "    features = vect.get_feature_names()\n",
    "    \n",
    "    scores = zip(vect.get_feature_names(),\n",
    "                 np.asarray(dtm.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    sorted_scores = dict(sorted_scores)\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sentence Score </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_score(text,sorted_scores):\n",
    "    sentence_score = []\n",
    "    for each in text:\n",
    "        score = 0.0\n",
    "    #     each = each.\n",
    "        for word in each.split(\" \"):\n",
    "            #print word\n",
    "            word = word.lower()\n",
    "            if word in sorted_scores:\n",
    "    #             print \"Hello\"\n",
    "                score += sorted_scores[word]\n",
    "    #             print word_scores[word]\n",
    "        sentence_score.append([each,score])    \n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TextRank </h2>\n",
    "<h3> Sentence Sim </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "#     print vector1,vector2\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Similarity with Word2Vec </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_scores(text):\n",
    "    v1 = np.zeros(150)\n",
    "    scores = []\n",
    "    model = Word2Vec.load('model')\n",
    "    for s in text:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        sentence = tokenizer.tokenize(s)\n",
    "        for w in sentence:\n",
    "            if w in model:\n",
    "                v1 = v1 + model[w]\n",
    "        scores.append([s,v1])\n",
    "    \n",
    "    from nltk.cluster.util import cosine_distance\n",
    "\n",
    "    sim = np.zeros([len(scores),len(scores)])\n",
    "    for i in xrange(len(scores)):\n",
    "        for j in xrange(len(scores)):   \n",
    "            sim[i][j] = cosine_distance(scores[i][1],scores[j][1])\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Build Similarity Matrix </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stopwords)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PageRank </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        delta = abs((new_P - P).sum())\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TextRank </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textrank(sentences,method, top_n=4, stopwords=None):\n",
    "    \"\"\"\n",
    "    sentences = a list of sentences [[w11, w12, ...], [w21, w22, ...], ...]\n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    if method == \"word_count\":\n",
    "        S = build_similarity_matrix(sentences, stopwords)\n",
    "    elif method == \"word2vec\":    \n",
    "        S = sim_scores(sentences)\n",
    "\n",
    "    sentence_ranks = pagerank(S)\n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(sentences)\n",
    "    machine_summary = \"\"\n",
    "    for each in summary:\n",
    "        machine_summary = machine_summary+each+\" \"\n",
    "    return machine_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Calculate Rouge </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rougeN(gold_summary,machine_summary,n):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    gold_summary_slammed = \"\"\n",
    "    for word in gold_summary.split():\n",
    "        gold_summary_slammed = gold_summary_slammed+stemmer.stem(word)+\" \"\n",
    "\n",
    "\n",
    "    machine_summary_slammed = \"\"\n",
    "    for word in machine_summary.split():\n",
    "        machine_summary_slammed = machine_summary_slammed+stemmer.stem(word)+\" \"\n",
    "\n",
    "    n_gold = ngrams(gold_summary_slammed.split(\" \"),n)\n",
    "    n_machine = ngrams(machine_summary_slammed.split(\" \"),n)\n",
    "\n",
    "    gold_list = []\n",
    "\n",
    "    for gram in n_gold:\n",
    "        gold_list.append(gram)\n",
    "\n",
    "    machine_list = []\n",
    "    for gram in n_machine:\n",
    "        machine_list.append(gram)\n",
    "\n",
    "    return float(len(list(set(gold_list).intersection(machine_list))))/float(len(list(set(gold_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Find Summary </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def summary(SUMMARY_COMP_FACT,sentences):\n",
    "#     scores = []\n",
    "#     top_sentences = sorted(dict(sentences).items(), key=lambda x: x[1], reverse=True)[:(len(sentences)/SUMMARY_COMP_FACT)]\n",
    "#     machine_summary = \"\"\n",
    "#     for each in top_sentences:\n",
    "#         machine_summary = machine_summary + each[0]\n",
    "#     scores.append([SUMMARY_COMP_FACT, len(top_sentences), rougeN(gold_summary,machine_summary,2), rougeN(gold_summary,machine_summary,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(sentences):\n",
    "    paired_sens = {}\n",
    "    num_sen = len(sentences)/6\n",
    "\n",
    "    for pair in enumerate(sentences):\n",
    "         paired_sens[pair[0]]=pair[1][1]\n",
    "\n",
    "    temp = sorted((paired_sens).items(), key=lambda x: x[1], reverse=True)[:int(num_sen)]\n",
    "    temp = sorted(dict(temp).items(), key=lambda x: x[0], reverse=False)[:int(num_sen)]\n",
    "    machine_summary = \"\"\n",
    "    for i in temp:\n",
    "#         print sentences[i[0]][0],\"\\n\"\n",
    "        machine_summary = machine_summary +  sentences[i[0]][0] + \"\\n\"\n",
    "    return machine_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Main </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashwat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/shashwat/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn's Approach    \t0.220338983051\t0.54347826087\n",
      "TextRaNk - Word Count\t0.169491525424\t0.5\n",
      "TextRank - Word2Vec\t0.474576271186\t0.826086956522\n",
      "1\n",
      "Luhn's Approach    \t0.133333333333\t0.392857142857\n",
      "TextRaNk - Word Count\t0.0333333333333\t0.267857142857\n",
      "TextRank - Word2Vec\t0.0666666666667\t0.267857142857\n",
      "2\n",
      "Luhn's Approach    \t0.216666666667\t0.384615384615\n",
      "TextRaNk - Word Count\t0.1\t0.326923076923\n",
      "TextRank - Word2Vec\t0.266666666667\t0.403846153846\n",
      "3\n",
      "UnicodeDecodeError\n",
      "4\n",
      "Luhn's Approach    \t0.372881355932\t0.653846153846\n",
      "TextRaNk - Word Count\t0.0\t0.346153846154\n",
      "TextRank - Word2Vec\t0.35593220339\t0.653846153846\n",
      "5\n",
      "Luhn's Approach    \t0.237288135593\t0.586956521739\n",
      "TextRaNk - Word Count\t0.0508474576271\t0.304347826087\n",
      "TextRank - Word2Vec\t0.254237288136\t0.586956521739\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "\n",
    "df = pd.read_csv('news_summary.csv')\n",
    "result = []\n",
    "# f = open(\"results.csv\",\"w+\")\n",
    "# f.write(\"article No. \\t Luhn Rouge1 \\t Luhn Rouge2 \\t TextRank-Word_count Rouge1 \\t TextRank-Word_count Rouge2 \\t TextRank-word2vec Rouge1 \\t TextRank-word2vec Rouge2\")\n",
    "for i in range(0,):\n",
    "    print i\n",
    "#     f.write(\"\\n\")\n",
    "    try:\n",
    "        url = df['read_more'][i]\n",
    "        gold_summary = df['text'][i]\n",
    "        if \"indiatoday\" in url or \"intoday\" in url:\n",
    "            baseurl = \"i\"\n",
    "        elif \"hindustantimes\" in url:\n",
    "            baseurl = \"h\"\n",
    "        elif \"theguardian\" in url:\n",
    "            baseurl = \"t\"\n",
    "\n",
    "        file = urllib.urlopen(url)\n",
    "        html = file.read()\n",
    "\n",
    "        text = clean_data(html,baseurl)\n",
    "        ws = word_scores(text)\n",
    "\n",
    "        ss = sentence_score(text,ws)\n",
    "\n",
    "        machine_summary_1 = summarize(ss)\n",
    "        summary = []\n",
    "        # TextRank with Word Count\n",
    "        machine_summary_2 = textrank(text, \"word_count\",len(text)/5,stopwords=stopwords.words('english'))\n",
    "        # TextRank with Word2Vec\n",
    "        machine_summary_3 = textrank(text, \"word2vec\",len(text)/5,stopwords=stopwords.words('english'))\n",
    "#         print \"Gold Summary:\",gold_summary,\"\\n\",\"\\n\"\n",
    "#         print \"Luhn + TFIDF:\",machine_summary_1,\"\\n\",\"\\n\",\"\\n\"\n",
    "#         print \"TextRank + WOrdCount:\",machine_summary_2,\"\\n\",\"\\n\",\"\\n\"\n",
    "\n",
    "#         print \"TextRank + Word2Vec:\",machine_summary_3,\"\\n\",\"\\n\",\"\\n\"\n",
    "        \n",
    "#         print \"\\n\"\n",
    "#         print i\n",
    "        print \"Luhn's Approach    \"+ \"\\t\" +str(rougeN(gold_summary,machine_summary_1,2))+\"\\t\"+str(rougeN(gold_summary,machine_summary_1,1))\n",
    "        print \"TextRaNk - Word Count\"+ \"\\t\" + str(rougeN(gold_summary,machine_summary_2,2))+ \"\\t\" +str(rougeN(gold_summary,machine_summary_2,1))\n",
    "        print \"TextRank - Word2Vec\"+ \"\\t\" + str(rougeN(gold_summary,machine_summary_3,2))+ \"\\t\" +str(rougeN(gold_summary,machine_summary_3,1))\n",
    "\n",
    "#         f.write(str(i)+\"\\t\" +str(rougeN(gold_summary,machine_summary_1,1))+\"\\t\"+str(rougeN(gold_summary,machine_summary_1,2)))\n",
    "#         f.write(\"\\t\" + str(rougeN(gold_summary,machine_summary_2,1))+ \"\\t\" +str(rougeN(gold_summary,machine_summary_2,2)))\n",
    "#         f.write(\"\\t\" + str(rougeN(gold_summary,machine_summary_3,1))+ \"\\t\" +str(rougeN(gold_summary,machine_summary_3,2)))\n",
    "\n",
    "#         result.append([i, rougeN(gold_summary,machine_summary,2),rougeN(gold_summary,machine_summary,1)])\n",
    "    except:\n",
    "        print \"UnicodeDecodeError\"\n",
    "        \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&utm_medium=referral&utm_campaign=fullarticle \n"
     ]
    }
   ],
   "source": [
    "print url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
